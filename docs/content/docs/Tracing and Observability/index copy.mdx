---
title: Tracing and Observability
description: Monitor agent performance, track costs, and debug with comprehensive tracing and observability features.
---

<h1 className="mt-4 text-3xl font-medium">
  <span className="text-primary">Tracing and Observability</span>
</h1>
<h3 className="-mt-7 text-lg font-normal text-muted-foreground">
  Monitor agent performance, track costs, and debug with comprehensive tracing
</h3>

Tracing gives you complete visibility into what your agents are doing. Track LLM calls, tool executions, token usage, API costs, and performance metrics in real-time.

## Why Tracing?

Production AI applications need observability:

* **Cost Control** - Track token usage and API costs per request
* **Performance Monitoring** - Measure latency and identify bottlenecks
* **Debugging** - See exactly what happened when something fails
* **Usage Analytics** - Understand how agents and tools are being used
* **Optimization** - Identify expensive operations and optimize them

## Quick Start

Enable tracing in one line:

```python
from peargent import create_agent
from peargent.telemetry import enable_tracing
from peargent.models import groq

# Enable tracing (in-memory storage by default) // [!code highlight:2]
tracer = enable_tracing()

# Create agent with tracing enabled
agent = create_agent(
    name="Assistant",
    description="Helpful assistant",
    persona="You are helpful",
    model=groq("llama-3.3-70b-versatile"),
    tracing=True  # Enable tracing for this agent
)

# Run agent - traces are automatically captured
result = agent.run("What is 2+2?")

# View traces
traces = tracer.list_traces()
print(f"Captured {len(traces)} traces")

# Print formatted traces
tracer.print_traces(limit=5, format="terminal")

# Print summary with costs
tracer.print_summary()
```

Output:
```
Captured 1 traces

================================================================================
TRACE SUMMARY
================================================================================
Total Traces: 1
Total Tokens: 127 (prompt: 89, completion: 38)
Total Cost: $0.000082
Average Cost per Trace: $0.000082
```

## What Gets Traced?

Every agent execution creates a **Trace** containing multiple **Spans**:

### **Trace**
A complete agent execution from start to finish:
- **Trace ID**: Unique identifier
- **Agent Name**: Which agent executed
- **Total Tokens**: All tokens used
- **Total Cost**: Total API cost
- **Duration**: Total execution time
- **Spans**: Individual operations within the trace

### **Spans**
Individual operations within a trace:
- **LLM Calls**: Model inference, tokens, cost, latency
- **Tool Executions**: Tool name, inputs, outputs, duration
- **Agent Reasoning**: Thinking steps, decisions

## Storage Options

### In-Memory (Default)
Perfect for development and testing:

```python
tracer = enable_tracing()  # In-memory storage
```

**Pros:** Fast, no setup required
**Cons:** Data lost when program exits

### File-Based Storage
Persist traces to disk:

```python
tracer = enable_tracing(
    store_type="file",
    storage_dir="./traces"
)
```

**Pros:** Simple, human-readable JSON files
**Cons:** Not optimized for large-scale queries

### SQLite Storage
Local database storage:

```python
tracer = enable_tracing(
    store_type="sqlite",
    database_path="./traces.db"
)
```

**Pros:** Fast queries, good for single-server apps
**Cons:** Not suitable for distributed systems

### PostgreSQL Storage
Production-grade database storage:

```python
tracer = enable_tracing(
    store_type="postgres",
    connection_string="postgresql://user:pass@localhost/dbname"
)
```

**Pros:** Scalable, concurrent access, powerful queries
**Cons:** Requires PostgreSQL server

See **<u>[Database Storage](/docs/Tracing%20and%20Observability/database-storage)</u>** for detailed setup.

## Accessing Traces

### List All Traces
```python
traces = tracer.list_traces()

for trace in traces:
    print(f"Trace {trace.id}: {trace.agent_name}")
    print(f"  Tokens: {trace.total_tokens}")
    print(f"  Cost: ${trace.total_cost:.6f}")
    print(f"  Duration: {trace.duration_ms}ms")
```

### Get Specific Trace
```python
trace = tracer.get_trace(trace_id="abc123")

# Access spans
for span in trace.spans:
    print(f"Span: {span.span_type}")
    print(f"  Model: {span.model}")
    print(f"  Tokens: {span.token_prompt} + {span.token_completion}")
    print(f"  Cost: ${span.cost:.6f}")
```

### Filter Traces
```python
# Get traces by agent name
agent_traces = [t for t in tracer.list_traces() if t.agent_name == "Researcher"]

# Get expensive traces
expensive = [t for t in tracer.list_traces() if t.total_cost > 0.001]

# Get recent traces
import datetime
recent = [t for t in tracer.list_traces()
          if t.timestamp > datetime.datetime.now() - datetime.timedelta(hours=1)]
```

## Formatted Output

### Terminal Format
```python
tracer.print_traces(limit=10, format="terminal")
```

Output:
```
================================================================================
TRACES (Showing 10 most recent)
================================================================================
1. Trace ID: abc123
   Agent: Assistant
   Tokens: 127 (prompt: 89, completion: 38)
   Cost: $0.000082
   Duration: 1245ms

2. Trace ID: def456
   Agent: Researcher
   Tokens: 456 (prompt: 234, completion: 222)
   Cost: $0.000295
   Duration: 2340ms
```

### Summary Statistics
```python
tracer.print_summary()
```

Output:
```
================================================================================
TRACE SUMMARY
================================================================================
Total Traces: 25
Total Tokens: 12,456 (prompt: 7,234, completion: 5,222)
Total Cost: $0.008123
Average Cost per Trace: $0.000325
Average Tokens per Trace: 498

Top Agents by Usage:
  1. Researcher: 12 traces ($0.004561)
  2. Writer: 8 traces ($0.002134)
  3. Analyst: 5 traces ($0.001428)
```

## Cost Tracking

Peargent automatically calculates costs based on model pricing:

```python
tracer = enable_tracing()

agent = create_agent(
    name="CostTest",
    description="Test cost tracking",
    persona="Be concise",
    model=groq("llama-3.3-70b-versatile"),
    tracing=True
)

result = agent.run("What is 2+2?")

# Get cost information
traces = tracer.list_traces()
trace = traces[0]

print(f"Model: {trace.spans[0].model}")
print(f"Prompt tokens: {trace.spans[0].token_prompt}")
print(f"Completion tokens: {trace.spans[0].token_completion}")
print(f"Total cost: ${trace.total_cost:.6f}")
```

See **<u>[Cost Tracking](/docs/Tracing%20and%20Observability/cost-tracking)</u>** for detailed pricing information and cost optimization strategies.

## Tracing with Pools

Pools automatically trace all agent executions:

```python
from peargent import create_pool

tracer = enable_tracing()

pool = create_pool(
    agents=[researcher, writer, reviewer],
    router=my_router,
    tracing=True  # Enable tracing for entire pool
)

result = pool.run("Research AI and write a summary")

# View all agent traces
traces = tracer.list_traces()
print(f"Pool execution created {len(traces)} traces")
```

Each agent in the pool creates its own trace.

## Performance Impact

Tracing has minimal overhead:

| Storage Type | Overhead | Use Case |
|-------------|----------|----------|
| In-Memory | &lt;1ms | Development, testing |
| File | ~2-5ms | Single-user apps |
| SQLite | ~5-10ms | Local production apps |
| PostgreSQL | ~10-20ms | Multi-user production |

The performance cost is negligible compared to LLM API latency (typically 500-2000ms).

## Best Practices

1. **Always enable tracing in production** for debugging and cost control
2. **Use database storage** (SQLite/PostgreSQL) for persistent traces
3. **Monitor costs regularly** using `tracer.print_summary()`
4. **Set up alerts** for unexpected cost spikes
5. **Archive old traces** to manage database size
6. **Use trace IDs** for debugging specific user requests
7. **Track costs per user** by filtering traces

## Disabling Tracing

### Globally
```python
from peargent.telemetry import disable_tracing

disable_tracing()  # Disable all tracing
```

### Per Agent
```python
agent = create_agent(
    name="NoTrace",
    description="Agent without tracing",
    persona="You help",
    model=groq(),
    tracing=False  # Disable tracing for this agent only
)
```

### Conditionally
```python
import os

# Enable only in production
tracer = enable_tracing() if os.getenv("ENV") == "production" else None
```

## What's Next?

**<u>[Cost Tracking](/docs/Tracing%20and%20Observability/cost-tracking)</u>**
Learn about model pricing, cost calculation, and optimization strategies.

**<u>[Database Storage](/docs/Tracing%20and%20Observability/database-storage)</u>**
Set up SQLite or PostgreSQL for persistent trace storage with custom table names.

## Common Use Cases

### 1. Cost Monitoring Dashboard
```python
tracer = enable_tracing(store_type="postgres", connection_string=DB_URL)

# Get daily costs
traces_today = [t for t in tracer.list_traces()
                if t.timestamp.date() == datetime.date.today()]
daily_cost = sum(t.total_cost for t in traces_today)
print(f"Today's cost: ${daily_cost:.2f}")
```

### 2. Performance Debugging
```python
# Find slow traces
slow_traces = [t for t in tracer.list_traces() if t.duration_ms > 5000]
for trace in slow_traces:
    print(f"Slow trace: {trace.id} took {trace.duration_ms}ms")
```

### 3. Usage Analytics
```python
# Agent usage statistics
from collections import Counter

agent_counts = Counter(t.agent_name for t in tracer.list_traces())
print("Agent usage:")
for agent, count in agent_counts.most_common():
    print(f"  {agent}: {count} executions")
```

### 4. A/B Testing
```python
# Compare costs between model versions
model_a_traces = [t for t in tracer.list_traces() if "gpt-4" in t.spans[0].model]
model_b_traces = [t for t in tracer.list_traces() if "llama" in t.spans[0].model]

avg_cost_a = sum(t.total_cost for t in model_a_traces) / len(model_a_traces)
avg_cost_b = sum(t.total_cost for t in model_b_traces) / len(model_b_traces)

print(f"GPT-4 avg cost: ${avg_cost_a:.6f}")
print(f"Llama avg cost: ${avg_cost_b:.6f}")
```
