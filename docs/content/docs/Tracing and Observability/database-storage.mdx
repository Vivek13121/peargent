---
title: Database Storage
description: Persist traces to SQLite or PostgreSQL for production-grade observability.
---

<h1 className="mt-4 text-3xl font-medium">
  <span className="text-primary">Database Storage</span>
</h1>
<h3 className="-mt-7 text-lg font-normal text-muted-foreground">
  Persist traces to SQLite or PostgreSQL for production-grade observability
</h3>

Database storage persists traces beyond program execution, enabling historical analysis, cost reporting, and production monitoring. Peargent supports multiple storage options: in-memory (default), file-based, SQLite, and PostgreSQL.

## Storage Options

Peargent provides four storage backends for traces:

### In-Memory (Default)
Perfect for development and testing:

```python
from peargent.telemetry import enable_tracing

tracer = enable_tracing()  # In-memory storage (default)
```

**Pros:** Fast, no setup required
**Cons:** Data lost when program exits

### File-Based Storage
Persist traces to disk as JSON files:

```python
tracer = enable_tracing(
    store_type="file",
    storage_dir="./traces"
)
```

**Pros:** Simple, human-readable JSON files
**Cons:** Not optimized for large-scale queries

### SQLite Storage
Local database storage:

```python
tracer = enable_tracing(
    store_type="sqlite",
    connection_string="sqlite:///./traces.db"
)
```

**Pros:** Fast queries, good for single-server apps
**Cons:** Not suitable for distributed systems

### PostgreSQL Storage
Production-grade database storage:

```python
tracer = enable_tracing(
    store_type="postgres",
    connection_string="postgresql://user:pass@localhost/dbname"
)
```

**Pros:** Scalable, concurrent access, powerful queries
**Cons:** Requires PostgreSQL server

## Storage Comparison

| Feature | In-Memory | File | SQLite | PostgreSQL |
|---------|-----------|------|--------|------------|
| Persistence | âŒ | âœ… | âœ… | âœ… |
| Query Performance | âš¡ Fastest | ðŸŒ Slow | ðŸš€ Fast | ðŸš€ Fast |
| Concurrent Access | âŒ | âŒ | âš ï¸ Limited | âœ… Excellent |
| Production Ready | âŒ | âŒ | âš ï¸ Single-server | âœ… Yes |
| Setup Required | âŒ None | âŒ None | âŒ None | âœ… Server needed |
| Use Case | Development | Testing | Local production | Multi-user production |

## Performance Impact

Tracing has minimal overhead across all storage types:

| Storage Type | Overhead | Use Case |
|-------------|----------|----------|
| In-Memory | &lt;1ms | Development, testing |
| File | ~2-5ms | Single-user apps |
| SQLite | ~5-10ms | Local production apps |
| PostgreSQL | ~10-20ms | Multi-user production |

The performance cost is negligible compared to LLM API latency (typically 500-2000ms).

## SQLite Storage

SQLite is perfect for development and single-server production deployments. No server setup required - just specify a file path.

### Basic Setup

```python
from peargent import create_agent
from peargent.telemetry import enable_tracing
from peargent.models import groq

# Enable SQLite tracing
tracer = enable_tracing(
    store_type="sqlite",
    connection_string="sqlite:///./traces.db"
)

agent = create_agent(
    name="calculator_agent",
    description="Calculator agent",
    persona="You are helpful",
    model=groq("llama-3.3-70b-versatile"),
    tracing=True
)

result = agent.run("What is 15 + 27?")

# Traces are automatically saved to traces.db
```

### Connection String Format

```python
# Local file in current directory
connection_string="sqlite:///./traces.db"

# Absolute path
connection_string="sqlite:////path/to/traces.db"

# In-memory SQLite (not persisted)
connection_string="sqlite:///:memory:"
```

### Custom Table Names

Customize table names for your database schema:

```python
tracer = enable_tracing(
    store_type="sqlite",
    connection_string="sqlite:///./traces.db",
    traces_table="my_traces",      # Default: "traces"
    spans_table="my_spans"          # Default: "spans"
)
```

### Querying SQLite Directly

Access the SQLite database using any SQLite client:

```bash
# Command line
sqlite3 traces.db "SELECT agent_name, COUNT(*) FROM traces GROUP BY agent_name;"

# Python
import sqlite3

conn = sqlite3.connect("traces.db")
cursor = conn.cursor()

cursor.execute("SELECT * FROM traces WHERE total_cost > 0.001")
expensive_traces = cursor.fetchall()

conn.close()
```

### Complete Example

```python
from peargent import create_agent, create_tool
from peargent.telemetry import enable_tracing
from peargent.models import groq


def calculator_func(operation: str, a: float, b: float) -> float:
    """Simple calculator tool."""
    operations = {
        "add": lambda x, y: x + y,
        "subtract": lambda x, y: x - y,
        "multiply": lambda x, y: x * y,
        "divide": lambda x, y: x / y if y != 0 else float('inf')
    }
    return operations.get(operation, lambda x, y: 0)(a, b)


calculator = create_tool(
    name="calculator",
    description="Performs basic arithmetic operations",
    input_parameters={"operation": str, "a": float, "b": float},
    call_function=calculator_func,
)

# Enable SQLite tracing
tracer = enable_tracing(
    store_type="sqlite",
    connection_string="sqlite:///./traces.db"
)

# Create agent
agent = create_agent(
    name="calculator_agent",
    description="A helpful calculator agent",
    persona="You are a helpful calculator. Use the calculator tool.",
    model=groq("llama-3.3-70b-versatile"),
    tools=[calculator],
    tracing=True
)

# Run tasks (automatically traced)
result1 = agent.run("What is 15 + 27?")
result2 = agent.run("What is 100 divided by 4?")

# Query traces
traces = tracer.list_traces()
print(f"Total traces: {len(traces)}")

for trace in traces:
    print(f"  {trace.agent_name}: ${trace.total_cost:.6f}")
```

## PostgreSQL Storage

PostgreSQL is recommended for production environments with multiple servers, high traffic, or advanced querying needs.

### Prerequisites

Install PostgreSQL driver:

```bash
pip install psycopg2-binary
```

Create database:

```sql
CREATE DATABASE traces_db;
```

### Basic Setup

```python
from peargent import create_agent
from peargent.telemetry import enable_tracing
from peargent.models import groq

# Enable PostgreSQL tracing
tracer = enable_tracing(
    store_type="postgres",
    connection_string="postgresql://postgres:password@localhost:5432/traces_db"
)

agent = create_agent(
    name="data_agent",
    description="Data analysis agent",
    persona="You are helpful",
    model=groq("llama-3.3-70b-versatile"),
    tracing=True
)

result = agent.run("Analyze this data...")

# Traces are automatically saved to PostgreSQL
```

### Connection String Format

```python
# Format: postgresql://username:password@host:port/database

# Local PostgreSQL
connection_string="postgresql://postgres:password@localhost:5432/traces_db"

# Remote PostgreSQL
connection_string="postgresql://user:pass@db.example.com:5432/mydb"

# With SSL
connection_string="postgresql://user:pass@db.example.com:5432/mydb?sslmode=require"
```

### Environment Variables for Security

Never hardcode credentials in production:

```python
import os

connection_string = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:password@localhost:5432/traces_db"
)

tracer = enable_tracing(
    store_type="postgres",
    connection_string=connection_string
)
```

Set environment variable:

```bash
export DATABASE_URL="postgresql://user:pass@host:5432/dbname"
```

### Custom Table Names

```python
tracer = enable_tracing(
    store_type="postgres",
    connection_string=connection_string,
    traces_table="production_traces",
    spans_table="production_spans"
)
```

### Schema Migration

Peargent automatically creates tables on first run:

```python
tracer = enable_tracing(
    store_type="postgres",
    connection_string=connection_string,
    auto_migrate=True  # Default: creates schema automatically
)
```

**Tables created:**
- `traces` - Stores trace metadata
- `spans` - Stores individual operations

### Querying PostgreSQL Directly

Access PostgreSQL using any PostgreSQL client:

```bash
# Command line
psql -d traces_db -c "SELECT agent_name, COUNT(*) FROM traces GROUP BY agent_name;"

# With connection string
psql postgresql://user:pass@host:5432/dbname \
  -c "SELECT * FROM traces WHERE total_cost > 0.001;"
```

### Complete Example

```python
import os
from peargent import create_agent, create_tool
from peargent.models import groq
from peargent.telemetry import enable_tracing


def data_analyzer_func(operation: str, data: str) -> str:
    """Mock data analysis tool."""
    operations = {
        "count_words": lambda d: f"Word count: {len(d.split())}",
        "count_chars": lambda d: f"Character count: {len(d)}",
        "uppercase": lambda d: d.upper(),
        "lowercase": lambda d: d.lower()
    }
    return operations.get(operation, lambda d: "Unknown operation")(data)


data_analyzer = create_tool(
    name="data_analyzer",
    description="Analyzes and processes text data",
    input_parameters={"operation": str, "data": str},
    call_function=data_analyzer_func,
)

# PostgreSQL connection from environment
connection_string = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:password@localhost:5432/traces_db"
)

# Enable PostgreSQL tracing
tracer = enable_tracing(
    store_type="postgres",
    connection_string=connection_string
)

# Create agent
agent = create_agent(
    name="data_analysis_agent",
    description="A data analysis assistant",
    persona="You are a data analyst. Use the data_analyzer tool.",
    model=groq("llama-3.3-70b-versatile"),
    tools=[data_analyzer],
    tracing=True
)

# Run tasks
result1 = agent.run("Count words in: 'Hello world from PostgreSQL'")
result2 = agent.run("Convert to uppercase: 'postgresql is awesome'")

# Query traces
traces = tracer.list_traces()
print(f"Total traces in PostgreSQL: {len(traces)}")

for trace in traces:
    print(f"  {trace.agent_name}")
    print(f"    Cost: ${trace.total_cost:.6f}")
    print(f"    Duration: {trace.duration_ms}ms")
```

## Database Schema

Both SQLite and PostgreSQL use the same schema:

### Traces Table

```sql
CREATE TABLE traces (
    id VARCHAR(36) PRIMARY KEY,              -- Trace UUID
    agent_name VARCHAR(255) NOT NULL,        -- Agent name
    input_data TEXT,                         -- Input query/data
    output TEXT,                             -- Agent output
    error TEXT,                              -- Error message (if any)
    session_id VARCHAR(255),                 -- Session identifier
    user_id VARCHAR(255),                    -- User identifier
    start_time TIMESTAMP NOT NULL,           -- Start timestamp
    end_time TIMESTAMP,                      -- End timestamp
    duration_ms INTEGER,                     -- Duration in milliseconds
    total_tokens INTEGER DEFAULT 0,          -- Total tokens used
    total_cost FLOAT DEFAULT 0.0,            -- Total cost in USD
    created_at TIMESTAMP DEFAULT NOW()       -- Record creation time
);

-- Indexes for performance
CREATE INDEX idx_traces_agent ON traces(agent_name);
CREATE INDEX idx_traces_session ON traces(session_id);
CREATE INDEX idx_traces_user ON traces(user_id);
CREATE INDEX idx_traces_start_time ON traces(start_time);
```

### Spans Table

```sql
CREATE TABLE spans (
    id VARCHAR(36) PRIMARY KEY,              -- Span UUID
    trace_id VARCHAR(36) NOT NULL,           -- Parent trace ID
    parent_span_id VARCHAR(36),              -- Parent span (for nesting)
    span_type VARCHAR(50) NOT NULL,          -- Type: llm, tool, agent
    name VARCHAR(255) NOT NULL,              -- Span name
    start_time TIMESTAMP NOT NULL,           -- Start timestamp
    end_time TIMESTAMP,                      -- End timestamp
    duration_ms INTEGER,                     -- Duration in milliseconds

    -- LLM-specific fields
    llm_prompt TEXT,                         -- LLM prompt
    llm_response TEXT,                       -- LLM response
    llm_model VARCHAR(100),                  -- Model name
    prompt_tokens INTEGER,                   -- Prompt tokens
    completion_tokens INTEGER,               -- Completion tokens
    cost FLOAT,                              -- Span cost

    -- Tool-specific fields
    tool_name VARCHAR(255),                  -- Tool name
    tool_args TEXT,                          -- Tool arguments (JSON)
    tool_output TEXT,                        -- Tool output

    created_at TIMESTAMP DEFAULT NOW(),      -- Record creation time

    FOREIGN KEY (trace_id) REFERENCES traces(id) ON DELETE CASCADE
);

-- Indexes for performance
CREATE INDEX idx_spans_trace ON spans(trace_id);
CREATE INDEX idx_spans_type ON spans(span_type);
```

## Advanced Queries

### Cost Analysis

```sql
-- Total cost by agent
SELECT
    agent_name,
    COUNT(*) as executions,
    SUM(total_cost) as total_cost,
    AVG(total_cost) as avg_cost
FROM traces
GROUP BY agent_name
ORDER BY total_cost DESC;
```

### Performance Analysis

```sql
-- Slowest traces
SELECT
    id,
    agent_name,
    duration_ms,
    total_cost
FROM traces
ORDER BY duration_ms DESC
LIMIT 10;
```

### Daily Costs

```sql
-- Cost per day
SELECT
    DATE(start_time) as date,
    SUM(total_cost) as daily_cost,
    COUNT(*) as executions
FROM traces
GROUP BY DATE(start_time)
ORDER BY date DESC;
```

### User Activity

```sql
-- Most active users
SELECT
    user_id,
    COUNT(*) as requests,
    SUM(total_cost) as total_cost
FROM traces
WHERE user_id IS NOT NULL
GROUP BY user_id
ORDER BY requests DESC
LIMIT 10;
```

### LLM Usage by Model

```sql
-- Token usage by model
SELECT
    llm_model,
    COUNT(*) as calls,
    SUM(prompt_tokens) as total_prompt_tokens,
    SUM(completion_tokens) as total_completion_tokens,
    SUM(cost) as total_cost
FROM spans
WHERE span_type = 'llm'
GROUP BY llm_model
ORDER BY total_cost DESC;
```

## Database Maintenance

### Archive Old Traces

Delete traces older than 30 days:

```python
import datetime

# SQLite
import sqlite3
conn = sqlite3.connect("traces.db")
cursor = conn.cursor()

thirty_days_ago = datetime.datetime.now() - datetime.timedelta(days=30)
cursor.execute("DELETE FROM traces WHERE start_time < ?", (thirty_days_ago,))
conn.commit()
conn.close()
```

```python
# PostgreSQL
import psycopg2

conn = psycopg2.connect(connection_string)
cursor = conn.cursor()

cursor.execute("""
    DELETE FROM traces
    WHERE start_time < NOW() - INTERVAL '30 days'
""")
conn.commit()
conn.close()
```

### Backup Database

```bash
# SQLite
cp traces.db traces_backup.db

# PostgreSQL
pg_dump -h localhost -U postgres -d traces_db > backup.sql
```

### Monitor Database Size

```sql
-- PostgreSQL: Check table sizes
SELECT
    table_name,
    pg_size_pretty(pg_total_relation_size(table_name::regclass)) as size
FROM information_schema.tables
WHERE table_schema = 'public'
ORDER BY pg_total_relation_size(table_name::regclass) DESC;
```

```bash
# SQLite: Check database size
ls -lh traces.db
```

## Best Practices

1. **Use environment variables** for database credentials in production
2. **Set up regular backups** for production databases
3. **Monitor database size** and archive old traces periodically
4. **Use custom table names** to avoid conflicts with existing schemas
5. **Add indexes** for frequently queried columns (user_id, session_id)
6. **Use PostgreSQL for production** with multiple servers or high traffic
7. **Use SQLite for development** or single-server deployments
8. **Enable auto_migrate** to automatically create schema on first run

## Troubleshooting

### PostgreSQL Connection Failed

```python
# Error: could not connect to server
# Solution: Check PostgreSQL is running
sudo service postgresql start  # Linux
brew services start postgresql  # macOS

# Error: database "traces_db" does not exist
# Solution: Create database
psql -U postgres -c "CREATE DATABASE traces_db;"
```

### SQLite Locked Database

```python
# Error: database is locked
# Solution: Close other connections or use WAL mode
import sqlite3
conn = sqlite3.connect("traces.db")
conn.execute("PRAGMA journal_mode=WAL")
conn.close()
```

### Missing psycopg2

```bash
# Error: No module named 'psycopg2'
# Solution: Install PostgreSQL driver
pip install psycopg2-binary
```

## What's Next?

**<u>[Tracing & Observability](/docs/Tracing%20and%20Observability)</u>**
Learn about tracing basics, accessing traces, and monitoring agent performance.

**<u>[Cost Tracking](/docs/Tracing%20and%20Observability/cost-tracking)</u>**
Understand model pricing, cost calculation, and optimization strategies.
